{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import warnings\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import data as data\n",
    "from data.BehavioralDataset import BehavioralDataset\n",
    "from data.BehavioralHmSamples import BehavioralHmSamples\n",
    "import scipy\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_netG(path, isize, nz, nc, ngf, n_extra_layers):\n",
    "    assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "    cngf, tisize = ngf//2, 4\n",
    "    while tisize != isize:\n",
    "        cngf = cngf * 2\n",
    "        tisize = tisize * 2\n",
    "\n",
    "    main = nn.Sequential()\n",
    "    # input is Z, going into a convolution\n",
    "    main.add_module('initial:{0}-{1}:convt'.format(nz, cngf),\n",
    "                    nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "    main.add_module('initial:{0}:batchnorm'.format(cngf),\n",
    "                    nn.BatchNorm2d(cngf))\n",
    "    main.add_module('initial:{0}:relu'.format(cngf),\n",
    "                    nn.ReLU(True))\n",
    "\n",
    "    csize, cndf = 4, cngf\n",
    "    while csize < isize//2:\n",
    "        main.add_module('pyramid:{0}-{1}:convt'.format(cngf, cngf//2),\n",
    "                        nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "        main.add_module('pyramid:{0}:batchnorm'.format(cngf//2),\n",
    "                        nn.BatchNorm2d(cngf//2))\n",
    "        main.add_module('pyramid:{0}:relu'.format(cngf//2),\n",
    "                        nn.ReLU(True))\n",
    "        cngf = cngf // 2\n",
    "        csize = csize * 2\n",
    "\n",
    "    # Extra layers\n",
    "    for t in range(n_extra_layers):\n",
    "        main.add_module('extra-layers-{0}:{1}:conv'.format(t, cngf),\n",
    "                        nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "        main.add_module('extra-layers-{0}:{1}:batchnorm'.format(t, cngf),\n",
    "                        nn.BatchNorm2d(cngf))\n",
    "        main.add_module('extra-layers-{0}:{1}:relu'.format(t, cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "    main.add_module('final:{0}-{1}:convt'.format(cngf, nc),\n",
    "                    nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "    main.add_module('final:{0}:tanh'.format(nc),\n",
    "                    nn.Tanh())\n",
    "    \n",
    "    state_dict = torch.load(path, map_location=torch.device('cpu'))\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[5:] # remove `main.`\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    main.load_state_dict(new_state_dict, strict=False)\n",
    "    \n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_netG_mlp(path, isize, nz, nc, ngf):\n",
    "    \n",
    "    main = nn.Sequential(\n",
    "        # Z goes into a linear of size: ngf\n",
    "        nn.Linear(nz, ngf),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(ngf, ngf),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(ngf, ngf),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(ngf, nc * isize * isize),\n",
    "    )\n",
    "    \n",
    "    state_dict = torch.load(path, map_location=torch.device('cpu'))\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[5:] # remove `main.`\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    main.load_state_dict(new_state_dict, strict=False)\n",
    "    \n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_netD(path, isize, nc, ndf, n_extra_layers):\n",
    "    \n",
    "    assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "    main = nn.Sequential()\n",
    "    # input is nc x isize x isize\n",
    "    main.add_module('initial:{0}-{1}:conv'.format(nc, ndf),\n",
    "                    nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "    main.add_module('initial:{0}:relu'.format(ndf),\n",
    "                    nn.LeakyReLU(0.2, inplace=True))\n",
    "    csize, cndf = isize / 2, ndf\n",
    "\n",
    "    # Extra layers\n",
    "    for t in range(n_extra_layers):\n",
    "        main.add_module('extra-layers-{0}:{1}:conv'.format(t, cndf),\n",
    "                        nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "        main.add_module('extra-layers-{0}:{1}:batchnorm'.format(t, cndf),\n",
    "                        nn.BatchNorm2d(cndf))\n",
    "        main.add_module('extra-layers-{0}:{1}:relu'.format(t, cndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "    while csize > 4:\n",
    "        in_feat = cndf\n",
    "        out_feat = cndf * 2\n",
    "        main.add_module('pyramid:{0}-{1}:conv'.format(in_feat, out_feat),\n",
    "                        nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "        main.add_module('pyramid:{0}:batchnorm'.format(out_feat),\n",
    "                        nn.BatchNorm2d(out_feat))\n",
    "        main.add_module('pyramid:{0}:relu'.format(out_feat),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        cndf = cndf * 2\n",
    "        csize = csize / 2\n",
    "\n",
    "    # state size. K x 4 x 4\n",
    "    main.add_module('final:{0}-{1}:conv'.format(cndf, 1),\n",
    "                    nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "\n",
    "    state_dict = torch.load(path, map_location=torch.device('cpu'))\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[5:] # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    main.load_state_dict(new_state_dict, strict=False)\n",
    "    return main\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_netD_mlp(path, isize, nc, ndf):\n",
    "    \n",
    "    main = nn.Sequential(\n",
    "            # Z goes into a linear of size: ndf\n",
    "            nn.Linear(nc * isize * isize, ndf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(ndf, ndf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(ndf, ndf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(ndf, 1),\n",
    "        )\n",
    "    \n",
    "    state_dict = torch.load(path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[5:] # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    main.load_state_dict(new_state_dict, strict=False)\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_mlp_input(input_sample):\n",
    "    return input_sample.view(input_sample.size(0), \n",
    "                             input_sample.size(1) * input_sample.size(2) * input_sample.size(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_wrapper(samples):\n",
    "    input = torch.FloatTensor(samples.shape[0], 1, 32, 32)\n",
    "    input.resize_as_(samples).copy_(samples)\n",
    "    return Variable(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in all needed data\n",
    "# load real samples\n",
    "dataset_cv = BehavioralDataset(isCnnData=True, isScoring=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset_cv, batch_size=3, shuffle=True, num_workers=1)\n",
    "data_iter = iter(dataloader)\n",
    "data = data_iter.next()\n",
    "real_samples, _ = data\n",
    "\n",
    "# load fake samples\n",
    "fake_samples_list = []\n",
    "for i in range(1,6):\n",
    "    dataset = BehavioralHmSamples(modelNum=i, isCnnData=True, isScoring=True)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1000, shuffle=True, num_workers=1)\n",
    "    data_iter = iter(dataloader)\n",
    "    data = data_iter.next()\n",
    "    next_samples, _ = data\n",
    "    fake_samples_list.append(next_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in discriminators \n",
    "num_nets = 30\n",
    "netD_list = [load_netD('./loss_curves/netD_50k_{0}_automated.pth'.format(i), isize=32, nc=1, ndf=64, n_extra_layers=0) for i in range(num_nets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score hierarchical models \n",
    "num_h_models = 5\n",
    "scores = np.zeros((num_h_models, num_nets))\n",
    "\n",
    "for j, netD in enumerate(netD_list):\n",
    "    \n",
    "    # score real samples \n",
    "    real_samples_scores = netD(sample_wrapper(real_samples)).data.numpy()\n",
    "    \n",
    "    for i, fake_samples in enumerate(fake_samples_list):\n",
    "        \n",
    "        fake_samples_scores = netD(sample_wrapper(fake_samples)).data.numpy()\n",
    "        \n",
    "        # see how many fake samples were scored as more real than each real samples\n",
    "        num_right = sum([np.sum(fake_samples_scores < real_samples_scores[k]) for k in range(real_samples_scores.shape[0])])\n",
    "#         print('index: ({0}, {1})'.format(i,j))\n",
    "#         print(num_right)\n",
    "#         print(np.sum(fake_samples_scores < real_samples_scores[0]))\n",
    "#         print(np.sum(fake_samples_scores < real_samples_scores[1]))\n",
    "#         print(np.sum(fake_samples_scores < real_samples_scores[2]))\n",
    "#         print(num_right / (1000 * real_samples_scores.shape[0]))\n",
    "#         print('-----------------------')\n",
    "        scores[i,j] = num_right / (1000 * real_samples_scores.shape[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df.to_csv('./data/behavioral_model_scores.csv', index=False, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = list(np.mean(scores, axis=1))\n",
    "samp_se_list = list(scipy.stats.sem(scores, axis=1))\n",
    "var_list = list(np.var(scores, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\hat{p}$ is the probability of a dog getting shocked.\n",
    "\n",
    "$X_{1it}$ and $X_{2it}$ are the number of times a dog has respectively been shocked and avoided being shocked in previous trials.\n",
    "\n",
    "Model 1 \n",
    "\\begin{align*}\n",
    "\\hat{p}=\\sigma(\\beta_{1}+\\beta_{2}X_{1it}+\\beta_{3}X_{2it})\n",
    "\\end{align*}\n",
    "\n",
    "Model 2 \n",
    "\\begin{align*}\n",
    "\\hat{p}=exp(\\beta_{1}X_{1it}+\\beta_{2}X_{2it})\n",
    "\\end{align*}\n",
    "\n",
    "Model 3 \n",
    "\\begin{align*}\n",
    "\\hat{p}&=\\sigma(\\frac{\\alpha}{t}+\\gamma)&\\text{$t$ is the trial number}\n",
    "\\end{align*}\n",
    "\n",
    "Model 4 is the \"switch.\" \n",
    "\n",
    "Model 5 \n",
    "\\begin{align*}\n",
    "\\hat{p}&=\\frac{\\alpha}{t}&\\text{$t$ is the trial number}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\t 1 \n",
      "Mean\t 0.667177777777778\n",
      "SE\t 0.024067406051692514\n",
      "Var\t 0.016797960987654324\n",
      "------------------\n",
      "Model\t 2 \n",
      "Mean\t 0.7062666666666666\n",
      "SE\t 0.021775332570521916\n",
      "Var\t 0.013750788148148148\n",
      "------------------\n",
      "Model\t 3 \n",
      "Mean\t 0.5994777777777779\n",
      "SE\t 0.023751027044182103\n",
      "Var\t 0.01635922728395062\n",
      "------------------\n",
      "Model\t 4 \n",
      "Mean\t 0.7566777777777778\n",
      "SE\t 0.03867475633216149\n",
      "Var\t 0.043376366543209886\n",
      "------------------\n",
      "Model\t 5 \n",
      "Mean\t 0.5840888888888888\n",
      "SE\t 0.024412917321396405\n",
      "Var\t 0.017283725432098763\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "for mean, var, se, i in zip(means, var_list, samp_se_list, range(1,len(means)+1)):\n",
    "    print('Model\\t {0} \\nMean\\t {1}\\nSE\\t {2}\\nVar\\t {3}'.format(i, mean, se, var))\n",
    "    print('------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_scores = scores.copy()\n",
    "scaler = MinMaxScaler()\n",
    "scaled_scores = scaler.fit_transform(scaled_scores)\n",
    "scaled_scores_df = pd.DataFrame(scaled_scores)\n",
    "scaled_scores_df.to_csv('./data/behavioral_model_scaled_scores.csv', index=False, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_means = list(np.mean(scaled_scores, axis=1))\n",
    "scaled_samp_se_list = list(scipy.stats.sem(scaled_scores, axis=1))\n",
    "scaled_var_list = list(np.var(scaled_scores, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\t 1 \n",
      "Mean\t 0.514460045178651\n",
      "SE\t 0.05277298885496625\n",
      "Var\t 0.08076466222790538\n",
      "------------------\n",
      "Model\t 2 \n",
      "Mean\t 0.6847650441536497\n",
      "SE\t 0.042408250528907565\n",
      "Var\t 0.05215533167475507\n",
      "------------------\n",
      "Model\t 3 \n",
      "Mean\t 0.2499346024572722\n",
      "SE\t 0.04968043018215691\n",
      "Var\t 0.07157620914944088\n",
      "------------------\n",
      "Model\t 4 \n",
      "Mean\t 0.8158547428141666\n",
      "SE\t 0.06065434855074908\n",
      "Var\t 0.10668954994535694\n",
      "------------------\n",
      "Model\t 5 \n",
      "Mean\t 0.28147390527489147\n",
      "SE\t 0.06812138886255541\n",
      "Var\t 0.13457518499634114\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "for mean, var, se, i in zip(scaled_means, scaled_var_list, scaled_samp_se_list, range(1,len(scaled_means)+1)):\n",
    "    print('Model\\t {0} \\nMean\\t {1}\\nSE\\t {2}\\nVar\\t {3}'.format(i, mean, se, var))\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
